{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e93de95",
   "metadata": {},
   "source": [
    "### Gettingg Started with Langchain and Open AI (or Gemini API Key)\n",
    "\n",
    "This is a quick start in which we will see how to:\n",
    "- Get set up with LangChain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain : prompts templetes models and output parsers.\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith \n",
    "- Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a1f9328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9c99de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACKING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ebc5418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='models/gemini-2.0-flash' google_api_key=SecretStr('**********') client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001B71CEF38F0> default_metadata=() model_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3f0eed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and getting response\n",
    "\n",
    "response = llm.invoke(\"What is generative AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53a90eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI refers to a class of artificial intelligence algorithms that can **create new content**, such as text, images, music, audio, video, and even code.  It learns the underlying patterns and structures within a dataset and then uses that knowledge to generate new data that resembles the original but is not simply a copy.\n",
      "\n",
      "Here's a breakdown of key aspects:\n",
      "\n",
      "*   **Generative Models:** These models are trained on large datasets of existing content. They learn the probability distribution of the data, allowing them to generate new, unseen samples that are statistically similar to the training data.\n",
      "\n",
      "*   **Types of Content:** Generative AI can create a wide range of content, including:\n",
      "    *   **Text:**  Writing articles, poems, scripts, summaries, chatbots, code, and more.\n",
      "    *   **Images:** Generating realistic or artistic images from text prompts, creating variations of existing images, or upscaling low-resolution images.\n",
      "    *   **Audio:** Composing music, generating sound effects, creating realistic speech, or synthesizing new audio samples.\n",
      "    *   **Video:** Creating short video clips, animating images, or generating realistic scenes.\n",
      "    *   **Code:**  Writing code snippets, generating entire programs, or translating code between different languages.\n",
      "    *   **3D Models:** Creating 3D models of objects, scenes, or characters.\n",
      "    *   **Molecular Structures:** Designing new molecules for drug discovery or materials science.\n",
      "\n",
      "*   **Common Architectures:** Several architectures are commonly used in generative AI, including:\n",
      "    *   **Generative Adversarial Networks (GANs):** GANs involve two neural networks: a generator that creates new content and a discriminator that tries to distinguish between real and generated content. The generator and discriminator are trained in competition, leading to increasingly realistic generated content.\n",
      "    *   **Variational Autoencoders (VAEs):** VAEs learn a compressed representation of the input data, called a latent space. They can then sample from this latent space to generate new data points.\n",
      "    *   **Transformers:** Transformers are a powerful architecture that has revolutionized natural language processing and is now widely used in other domains, such as image generation.  Large Language Models (LLMs) like GPT-3, GPT-4, LaMDA, and Bard are based on the transformer architecture.\n",
      "    *   **Diffusion Models:** Diffusion models work by gradually adding noise to the data until it becomes pure noise, and then learning to reverse this process to generate new data from noise.  They are known for producing high-quality images.\n",
      "\n",
      "*   **How it Works (Simplified):** Imagine teaching a computer to draw cats. You feed it thousands of cat pictures. The generative AI learns the common features of cats (fur, whiskers, pointy ears, etc.).  Then, when you ask it to draw a new cat, it uses this knowledge to create an image that has those cat-like characteristics, even if it's a cat it's never seen before.\n",
      "\n",
      "*   **Applications:** Generative AI has numerous applications across various industries:\n",
      "    *   **Art and Design:** Creating artwork, generating design prototypes, and enhancing existing images.\n",
      "    *   **Entertainment:** Generating special effects for movies, creating video game content, and composing music.\n",
      "    *   **Marketing and Advertising:** Generating marketing copy, creating product images, and personalizing advertising campaigns.\n",
      "    *   **Healthcare:** Designing new drugs, generating medical images, and personalizing treatment plans.\n",
      "    *   **Education:** Creating personalized learning content, generating educational simulations, and providing automated feedback.\n",
      "    *   **Software Development:** Generating code, testing software, and automating software development tasks.\n",
      "    *   **Scientific Research:** Simulating scientific phenomena, discovering new materials, and analyzing data.\n",
      "\n",
      "*   **Limitations and Challenges:**\n",
      "    *   **Bias:** Generative AI models can inherit biases from the training data, leading to biased or discriminatory outputs.\n",
      "    *   **Control:** It can be challenging to control the specific characteristics of the generated content.\n",
      "    *   **Ethical Concerns:** Generative AI can be used to create deepfakes, spread misinformation, and automate tasks that were previously performed by humans.\n",
      "    *   **Copyright and Ownership:** The legal implications of using generative AI to create content are still being debated.\n",
      "    *   **Computational Resources:** Training generative AI models can require significant computational resources.\n",
      "\n",
      "In summary, generative AI is a powerful technology with the potential to transform many aspects of our lives. It's a rapidly evolving field, and we can expect to see even more impressive applications in the years to come.  However, it's also important to be aware of the limitations and ethical considerations associated with this technology.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da2e837",
   "metadata": {},
   "source": [
    "#### Chatprompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "951fc6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert AI Engineer. Provide me answers based on the question\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "971a8647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's dive into LangSmith. Here's a comprehensive overview of what LangSmith is, its purpose, key features, and how it fits into the broader landscape of LLM (Large Language Model) development and deployment.\n",
      "\n",
      "**What is LangSmith?**\n",
      "\n",
      "LangSmith is a unified platform designed to help developers build, test, debug, and monitor LLM-powered applications.  It's essentially a developer toolchain that streamlines the process of working with LLMs, addressing the unique challenges that arise when dealing with these complex AI models.  It is developed by LangChain.\n",
      "\n",
      "**Why is LangSmith Needed? (The Problem)**\n",
      "\n",
      "Developing applications that rely on LLMs is significantly different from traditional software development. Here's why:\n",
      "\n",
      "*   **Observability Challenges:** LLMs are \"black boxes\" to some extent.  It's hard to understand *why* an LLM produced a particular output. Traditional debugging methods are insufficient.\n",
      "*   **Evaluation Complexity:**  Measuring the quality of LLM output is subjective and context-dependent.  Simple metrics like accuracy don't always capture the nuances of a good response.\n",
      "*   **Iterative Development:** Building with LLMs is highly iterative. You need to experiment with different prompts, models, and parameters to achieve the desired results. This requires a robust system for tracking experiments and comparing performance.\n",
      "*   **Production Monitoring:**  LLM performance can degrade over time due to changes in the model, data, or user behavior.  Continuous monitoring is crucial to maintain application quality.\n",
      "\n",
      "LangSmith addresses these challenges by providing tools for observability, evaluation, testing, and monitoring specifically tailored for LLM applications.\n",
      "\n",
      "**Key Features and Functionality**\n",
      "\n",
      "LangSmith offers a range of features that cover the entire lifecycle of LLM application development:\n",
      "\n",
      "*   **Tracing and Debugging:**\n",
      "    *   **Detailed Traces:** Captures the entire execution flow of your LLM application, including inputs, outputs, intermediate steps, and latency information.  This allows you to pinpoint the source of errors or unexpected behavior.\n",
      "    *   **Visualizations:** Presents traces in a clear and intuitive way, making it easier to understand the flow of data and identify bottlenecks.\n",
      "    *   **Collaboration:** Enables teams to share traces and collaborate on debugging.\n",
      "\n",
      "*   **Evaluation and Testing:**\n",
      "    *   **Test Datasets:**  Allows you to create and manage test datasets for evaluating LLM performance.\n",
      "    *   **Evaluation Metrics:** Provides a variety of metrics for assessing LLM output, including accuracy, relevance, coherence, and fluency.\n",
      "    *   **Custom Evaluation:**  Enables you to define your own custom evaluation metrics to match the specific requirements of your application.\n",
      "    *   **Automated Testing:**  Supports automated testing of LLM applications, allowing you to catch regressions early in the development process.\n",
      "\n",
      "*   **Experiment Management:**\n",
      "    *   **Experiment Tracking:**  Tracks all your experiments, including the prompts, models, parameters, and evaluation results.\n",
      "    *   **Version Control:**  Provides version control for your prompts and models, allowing you to easily revert to previous versions.\n",
      "    *   **A/B Testing:**  Supports A/B testing of different prompts and models to optimize performance.\n",
      "\n",
      "*   **Monitoring and Observability:**\n",
      "    *   **Real-time Monitoring:**  Monitors the performance of your LLM application in real-time, providing insights into latency, error rates, and usage patterns.\n",
      "    *   **Alerting:**  Sends alerts when performance degrades or anomalies are detected.\n",
      "    *   **Data Visualization:**  Provides dashboards and visualizations to help you understand the behavior of your LLM application over time.\n",
      "    *   **Feedback Collection:** Allows you to collect user feedback on LLM outputs, providing valuable data for improving model performance.\n",
      "\n",
      "*   **Integration with LangChain:**\n",
      "    *   LangSmith is deeply integrated with LangChain, a popular framework for building LLM applications. This integration makes it easy to trace, debug, and evaluate LangChain chains and agents.\n",
      "    *   Seamlessly logs all the steps within LangChain components.\n",
      "\n",
      "**How LangSmith Works (Simplified Workflow)**\n",
      "\n",
      "1.  **Instrument Your Code:** You integrate the LangSmith SDK into your LLM application code.  This involves adding a few lines of code to wrap your LLM calls and other relevant operations.\n",
      "2.  **Run Your Application:**  As your application runs, LangSmith automatically captures traces, logs, and metrics.\n",
      "3.  **Analyze the Data:**  You use the LangSmith UI to analyze the collected data, identify issues, and evaluate performance.\n",
      "4.  **Iterate and Improve:**  Based on your analysis, you refine your prompts, models, and parameters, and repeat the process.\n",
      "\n",
      "**Benefits of Using LangSmith**\n",
      "\n",
      "*   **Faster Development:**  Speeds up the development process by providing tools for debugging, testing, and evaluating LLM applications.\n",
      "*   **Improved Quality:**  Helps you build higher-quality LLM applications by providing insights into model behavior and performance.\n",
      "*   **Reduced Costs:**  Reduces costs by identifying and fixing issues early in the development process.\n",
      "*   **Increased Confidence:**  Gives you more confidence in the reliability and performance of your LLM applications.\n",
      "*   **Better Observability:** Provides deep insights into the inner workings of your LLM applications.\n",
      "*   **Data-Driven Decisions:** Enables you to make data-driven decisions about how to improve your LLM applications.\n",
      "\n",
      "**Who Should Use LangSmith?**\n",
      "\n",
      "LangSmith is valuable for:\n",
      "\n",
      "*   **LLM Application Developers:**  Anyone building applications that use large language models.\n",
      "*   **Data Scientists:**  Data scientists who are responsible for training and evaluating LLMs.\n",
      "*   **MLOps Engineers:**  MLOps engineers who are responsible for deploying and monitoring LLM applications.\n",
      "*   **Teams:** Teams collaborating on LLM projects.\n",
      "\n",
      "**LangSmith vs. Alternatives**\n",
      "\n",
      "While other tools provide some observability or monitoring features, LangSmith is unique in its focus on the specific challenges of LLM development and its deep integration with LangChain.  Alternatives might include:\n",
      "\n",
      "*   **General-Purpose Monitoring Tools:** (e.g., Prometheus, Grafana, Datadog):  These tools are good for monitoring infrastructure but lack the specialized features needed for LLM applications.\n",
      "*   **MLOps Platforms:** (e.g., MLflow, Kubeflow): These platforms provide tools for managing the ML lifecycle but may not have specific support for LLMs.\n",
      "*   **Custom Logging Solutions:**  Building your own logging and monitoring system is possible but requires significant effort and expertise.\n",
      "\n",
      "**In Summary**\n",
      "\n",
      "LangSmith is a powerful platform that addresses the unique challenges of building, testing, and deploying LLM-powered applications.  Its features for tracing, evaluation, experiment management, and monitoring can significantly improve the quality, reliability, and efficiency of LLM development. If you're working with LLMs, LangSmith is definitely worth exploring.\n"
     ]
    }
   ],
   "source": [
    "chain = prompt|llm\n",
    "\n",
    "response = chain.invoke({\"input\" : \"Can you tell me about Langsmith?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666805a1",
   "metadata": {},
   "source": [
    "#### StrOutput Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94be257e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's delve into LangSmith. I can provide you with a comprehensive overview of what it is, its key features, benefits, and how it fits within the broader LangChain ecosystem.\n",
      "\n",
      "**What is LangSmith?**\n",
      "\n",
      "LangSmith is a unified platform designed to help you **debug, test, evaluate, and monitor** your LangChain applications and LLM-powered systems.  In essence, it's a developer tool built to bridge the gap between experimentation and production for LLM applications. It helps you go beyond simple \"it works on my machine\" testing to ensure your LLM apps are reliable and performant in real-world scenarios.\n",
      "\n",
      "Think of it as a combination of:\n",
      "\n",
      "*   **Observability Platform:** Provides insights into the inner workings of your LLM application.\n",
      "*   **Testing and Evaluation Framework:** Enables you to systematically assess the quality and performance of your application.\n",
      "*   **Collaboration Tool:** Facilitates teamwork by allowing you to share and discuss results.\n",
      "\n",
      "**Key Features and Capabilities:**\n",
      "\n",
      "*   **Tracing and Debugging:**\n",
      "    *   **Detailed Traces:**  LangSmith captures the entire execution flow of your LangChain application, including LLM calls, tool usage, and intermediate steps.  You can see the exact inputs and outputs at each stage.\n",
      "    *   **Visualizations:**  Provides a visual representation of your chains and agents, making it easier to understand the flow of data and identify bottlenecks.\n",
      "    *   **Error Tracking:**  Helps you identify and diagnose errors within your application.\n",
      "*   **Testing and Evaluation:**\n",
      "    *   **Datasets:**  Allows you to create and manage datasets of inputs and expected outputs for your application.\n",
      "    *   **Automated Evaluation:**  Enables you to run your application against these datasets and automatically evaluate the results using metrics like accuracy, relevance, and coherence. You can define custom evaluation functions.\n",
      "    *   **LLM-Based Evaluation:** Leverage LLMs themselves to evaluate the quality of responses, providing more nuanced and human-like assessments.\n",
      "*   **Monitoring and Observability:**\n",
      "    *   **Performance Monitoring:**  Tracks key performance indicators (KPIs) like latency, token usage, and cost.\n",
      "    *   **Usage Tracking:**  Provides insights into how your application is being used by end-users.\n",
      "    *   **Alerting:**  Allows you to set up alerts for when certain metrics exceed predefined thresholds.\n",
      "*   **Collaboration:**\n",
      "    *   **Shared Projects:**  Enables teams to collaborate on projects and share results.\n",
      "    *   **Comments and Annotations:**  Allows you to add comments and annotations to traces and evaluations, facilitating discussions and knowledge sharing.\n",
      "*   **Integration with LangChain:**\n",
      "    *   **Seamless Integration:**  LangSmith is designed to work seamlessly with LangChain.  You can easily enable tracing and logging in your LangChain applications with minimal code changes.\n",
      "    *   **LangSmith as a Hub:**  Allows you to store and retrieve LangChain components (prompts, chains, agents) in a central location.\n",
      "\n",
      "**Benefits of Using LangSmith:**\n",
      "\n",
      "*   **Improved Reliability:**  Identify and fix bugs early in the development process, leading to more reliable applications.\n",
      "*   **Enhanced Performance:**  Optimize your application for speed and efficiency by identifying bottlenecks and areas for improvement.\n",
      "*   **Better Quality:**  Evaluate the quality of your application's outputs and ensure they meet your desired standards.\n",
      "*   **Faster Development:**  Debug and iterate on your application more quickly with detailed traces and automated evaluation.\n",
      "*   **Reduced Costs:**  Optimize your LLM usage and reduce costs by identifying areas where you can use less expensive models or prompts.\n",
      "*   **Increased Confidence:**  Deploy your applications with greater confidence, knowing that they have been thoroughly tested and evaluated.\n",
      "*   **Collaboration and Knowledge Sharing:** Facilitate team collaboration and knowledge sharing around LLM application development.\n",
      "\n",
      "**How LangSmith Fits into the LangChain Ecosystem:**\n",
      "\n",
      "LangSmith is a complementary tool to LangChain.  LangChain provides the building blocks for creating LLM applications (models, prompts, chains, agents), while LangSmith provides the tools for debugging, testing, evaluating, and monitoring those applications.\n",
      "\n",
      "Think of it this way:\n",
      "\n",
      "*   **LangChain:** The framework for *building* LLM applications.\n",
      "*   **LangSmith:** The platform for *operating* and *improving* LLM applications.\n",
      "\n",
      "You use LangChain to define the logic of your application, and then you use LangSmith to observe, test, and evaluate how that application is performing.  The integration between the two is very tight, making it easy to get started with LangSmith if you're already using LangChain.\n",
      "\n",
      "**Key Use Cases:**\n",
      "\n",
      "*   **Debugging Complex Chains and Agents:**  Troubleshooting issues in complex LangChain applications with multiple steps and dependencies.\n",
      "*   **Evaluating Different LLM Models:** Comparing the performance of different LLM models for a specific task.\n",
      "*   **Optimizing Prompts:**  Experimenting with different prompts to improve the accuracy and relevance of your application's outputs.\n",
      "*   **Building Robust Evaluation Pipelines:**  Creating automated evaluation pipelines to ensure the quality of your application over time.\n",
      "*   **Monitoring Production Applications:**  Tracking the performance and usage of your application in production and identifying potential issues.\n",
      "*   **A/B Testing:** Comparing different versions of your application to see which performs better.\n",
      "\n",
      "**How to Get Started with LangSmith:**\n",
      "\n",
      "1.  **Sign Up:** Create an account on the LangSmith platform.\n",
      "2.  **Install the LangChain Library:** Make sure you have the latest version of the LangChain library installed.\n",
      "3.  **Configure Environment Variables:** Set the necessary environment variables to connect your LangChain application to LangSmith.  Typically this involves setting `LANGCHAIN_TRACING_V2=true` and providing your LangSmith API key.\n",
      "4.  **Run Your LangChain Application:**  Run your LangChain application as you normally would.  LangSmith will automatically capture traces and logs.\n",
      "5.  **Explore the LangSmith UI:**  Use the LangSmith UI to explore the traces, evaluations, and other data that have been collected.\n",
      "\n",
      "**Example (Conceptual):**\n",
      "\n",
      "```python\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.chains import LLMChain\n",
      "from langchain.prompts import PromptTemplate\n",
      "\n",
      "# Set up LangSmith (requires API key and environment setup)\n",
      "# ...\n",
      "\n",
      "# Define your LangChain components\n",
      "prompt = PromptTemplate(\n",
      "    input_variables=[\"product\"],\n",
      "    template=\"What are 3 good things about {product}?\",\n",
      ")\n",
      "llm = OpenAI(temperature=0.7)\n",
      "chain = LLMChain(llm=llm, prompt=prompt)\n",
      "\n",
      "# Run the chain\n",
      "product_name = \"a new AI-powered toothbrush\"\n",
      "result = chain.run(product_name)\n",
      "\n",
      "print(result)\n",
      "\n",
      "# Now, go to the LangSmith UI to see the trace of this execution,\n",
      "# including the prompt, LLM call, and the final output.\n",
      "```\n",
      "\n",
      "**Important Considerations:**\n",
      "\n",
      "*   **Cost:** LangSmith is a paid service, so be sure to review the pricing plans before you start using it.\n",
      "*   **Data Privacy:**  Be aware of the data that you are sending to LangSmith and ensure that you are complying with all relevant data privacy regulations.\n",
      "*   **API Key Security:**  Protect your LangSmith API key and don't share it with anyone.\n",
      "\n",
      "In summary, LangSmith is a powerful platform that can help you build, test, and deploy high-quality LLM applications.  If you're serious about using LangChain in production, it's definitely worth checking out. It provides the necessary tools to move beyond simple experimentation and build robust, reliable, and performant LLM-powered systems.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt|llm|output_parser\n",
    "\n",
    "response = chain.invoke({\"input\" : \"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362ed7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang-c",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
